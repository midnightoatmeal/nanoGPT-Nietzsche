# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OGAdupWyIOzoDJe3NZ4mwzdCoKMBNo78
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import requests
import os
import requests

DATASET_PATH = "/content/thus_spoke_zarathustra.txt"

if not os.path.exists(DATASET_PATH):
    print("Downloading *Thus Spoke Zarathustra* dataset...")
    url = "https://www.gutenberg.org/cache/epub/1998/pg1998.txt"
    response = requests.get(url)
    with open(DATASET_PATH, "w", encoding="utf-8") as f:
        f.write(response.text)

print("âœ… Dataset ready!")

# ðŸ“Œ Load and preprocess dataset
with open(DATASET_PATH, "r", encoding="utf-8") as f:
    text = f.read()

# Tokenization (Character-Level)
chars = sorted(set(text))
print(chars)
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for ch, i in stoi.items()}
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

# Convert text to numerical sequence
data = torch.tensor(encode(text), dtype=torch.long)

# Split into training and validation sets
n = int(0.9 * len(data))
train_data, val_data = data[:n], data[n:]

# ðŸ“Œ Define a Minimal Transformer-Based GPT Model
class SimpleGPT(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm(x)
        x = self.fc(x)
        return x

# ðŸ“Œ Model Parameters
VOCAB_SIZE = len(chars)
EMBED_DIM = 256
HIDDEN_DIM = 512
BATCH_SIZE = 32
SEQ_LEN = 100
EPOCHS = 10
LR = 0.002

# ðŸ“Œ Prepare dataset
class TextDataset(Dataset):
    def __init__(self, data, seq_len):
        self.data = data
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.seq_len]
        y = self.data[idx + 1:idx + self.seq_len + 1]
        return x, y

train_dataset = TextDataset(train_data, SEQ_LEN)
val_dataset = TextDataset(val_data, SEQ_LEN)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

# ðŸ“Œ Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleGPT(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

# ðŸ“Œ Training loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        outputs = model(x)
        loss = criterion(outputs.view(-1, VOCAB_SIZE), y.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss / len(train_loader):.4f}")

# ðŸ“Œ Save model
torch.save(model.state_dict(), "nietzsche_gpt.pth")
print("âœ… Model training complete! Saved as 'nietzsche_gpt.pth'.")

